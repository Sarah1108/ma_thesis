{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a3eddf3",
   "metadata": {},
   "source": [
    "#### Logitic Regression with LeaveOneGroupOut Cross Validation using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5793fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import Libraries'''\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from joblib import load, Parallel, delayed\n",
    "import regex as re\n",
    "import pickle as pkl \n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Sampler, BatchSampler, Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "880c5caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data\n",
    "df = pd.read_pickle('data/dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52159c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>agenda</th>\n",
       "      <th>speechnumber</th>\n",
       "      <th>speaker</th>\n",
       "      <th>party</th>\n",
       "      <th>party.facts.id</th>\n",
       "      <th>chair</th>\n",
       "      <th>terms</th>\n",
       "      <th>text</th>\n",
       "      <th>parliament</th>\n",
       "      <th>iso3country</th>\n",
       "      <th>count_president</th>\n",
       "      <th>count_minister</th>\n",
       "      <th>count_scribe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2003-06-04</td>\n",
       "      <td>Aktuelle Stunde</td>\n",
       "      <td>5</td>\n",
       "      <td>Heidrun Silhavy</td>\n",
       "      <td>SPÖ</td>\n",
       "      <td>1384.0</td>\n",
       "      <td>False</td>\n",
       "      <td>308.0</td>\n",
       "      <td>Herr Präsident! Herr Vizekanzler! Herr Bundes...</td>\n",
       "      <td>AT-Nationalrat</td>\n",
       "      <td>AUS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2003-06-04</td>\n",
       "      <td>Aktuelle Stunde</td>\n",
       "      <td>7</td>\n",
       "      <td>Heidrun Silhavy</td>\n",
       "      <td>SPÖ</td>\n",
       "      <td>1384.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1385.0</td>\n",
       "      <td>Selbstverständlich, Herr Präsident. Ich nehme...</td>\n",
       "      <td>AT-Nationalrat</td>\n",
       "      <td>AUS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2003-06-04</td>\n",
       "      <td>Aktuelle Stunde</td>\n",
       "      <td>9</td>\n",
       "      <td>Herbert Haupt</td>\n",
       "      <td>FPÖ</td>\n",
       "      <td>463.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>Herr Präsident! Sehr geehrte Damen und Herren...</td>\n",
       "      <td>AT-Nationalrat</td>\n",
       "      <td>AUS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2003-06-04</td>\n",
       "      <td>Aktuelle Stunde</td>\n",
       "      <td>11</td>\n",
       "      <td>Walter Tancsits</td>\n",
       "      <td>ÖVP</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>False</td>\n",
       "      <td>638.0</td>\n",
       "      <td>Herr Präsident! Herr Vizekanzler! Frau Staats...</td>\n",
       "      <td>AT-Nationalrat</td>\n",
       "      <td>AUS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2003-06-04</td>\n",
       "      <td>Aktuelle Stunde</td>\n",
       "      <td>13</td>\n",
       "      <td>Doris Bures</td>\n",
       "      <td>SPÖ</td>\n",
       "      <td>1384.0</td>\n",
       "      <td>False</td>\n",
       "      <td>836.0</td>\n",
       "      <td>Sehr geehrter Herr Präsident! Meine sehr geeh...</td>\n",
       "      <td>AT-Nationalrat</td>\n",
       "      <td>AUS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date           agenda  speechnumber          speaker party  \\\n",
       "5   2003-06-04  Aktuelle Stunde             5  Heidrun Silhavy   SPÖ   \n",
       "7   2003-06-04  Aktuelle Stunde             7  Heidrun Silhavy   SPÖ   \n",
       "9   2003-06-04  Aktuelle Stunde             9    Herbert Haupt   FPÖ   \n",
       "11  2003-06-04  Aktuelle Stunde            11  Walter Tancsits   ÖVP   \n",
       "13  2003-06-04  Aktuelle Stunde            13      Doris Bures   SPÖ   \n",
       "\n",
       "    party.facts.id  chair   terms  \\\n",
       "5           1384.0  False   308.0   \n",
       "7           1384.0  False  1385.0   \n",
       "9            463.0  False  1562.0   \n",
       "11          1329.0  False   638.0   \n",
       "13          1384.0  False   836.0   \n",
       "\n",
       "                                                 text      parliament  \\\n",
       "5    Herr Präsident! Herr Vizekanzler! Herr Bundes...  AT-Nationalrat   \n",
       "7    Selbstverständlich, Herr Präsident. Ich nehme...  AT-Nationalrat   \n",
       "9    Herr Präsident! Sehr geehrte Damen und Herren...  AT-Nationalrat   \n",
       "11   Herr Präsident! Herr Vizekanzler! Frau Staats...  AT-Nationalrat   \n",
       "13   Sehr geehrter Herr Präsident! Meine sehr geeh...  AT-Nationalrat   \n",
       "\n",
       "   iso3country  count_president  count_minister  count_scribe  \n",
       "5          AUS                0               0             0  \n",
       "7          AUS                0               0             0  \n",
       "9          AUS                0               1             0  \n",
       "11         AUS                0               0             0  \n",
       "13         AUS                0               0             0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48af7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_f1(train_f1, dev_f1, threshold=0.0015):\n",
    "    \"\"\"\n",
    "    Returns development F1 if overfitting is below threshold, otherwise 0.\n",
    "    \"\"\"\n",
    "    return dev_f1 if (train_f1 - dev_f1) < threshold else 0\n",
    "\n",
    "\n",
    "def save_metrics(*args, path, fname):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    if not os.path.isfile(path + fname):\n",
    "        with open(path + fname, \"w\", newline=\"\\n\") as f:\n",
    "            f.write(\n",
    "                \",\".join(\n",
    "                    [\n",
    "                        \"config\",\n",
    "                        \"epoch\",\n",
    "                        \"train_loss\",\n",
    "                        \"train_acc\",\n",
    "                        \"train_f1\",\n",
    "                        \"val_loss\",\n",
    "                        \"val_acc\",\n",
    "                        \"val_f1\",\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            f.write(\"\\n\")\n",
    "    if args:\n",
    "        with open(path + fname, \"a\", newline=\"\\n\") as f:\n",
    "            f.write(\",\".join([str(arg) for arg in args]))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "603724bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5_000 #20_000\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 15\n",
    "MAX_LEN = 128 #256\n",
    "LEARNING_RATE = 1e-4\n",
    "seed_value = 43\n",
    "torch.manual_seed(seed_value)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4934d124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "german BERT \n",
    "'''\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
    "model = AutoModel.from_pretrained(\"dbmdz/bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703bd896",
   "metadata": {},
   "source": [
    "link german BERT hugging face [https://huggingface.co/dbmdz/bert-base-german-cased]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23c8e3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\sarah\\anaconda3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: boto3 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (1.21.32)\n",
      "Requirement already satisfied: requests in c:\\users\\sarah\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: regex in c:\\users\\sarah\\anaconda3\\lib\\site-packages (2024.9.11)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\sarah\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.32 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from boto3) (1.24.46)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from boto3) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from requests) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: click in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from sacremoses) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from sacremoses) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from botocore<1.25.0,>=1.24.32->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.32->boto3) (1.16.0)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "   ---------------------------------------- 897.5/897.5 kB 4.1 MB/s eta 0:00:00\n",
      "Installing collected packages: sacremoses\n",
      "Successfully installed sacremoses-0.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install tqdm boto3 requests regex sentencepiece sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "321fc8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\sarah\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from datasets) (0.25.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.0.1\n",
      "    Uninstalling datasets-3.0.1:\n",
      "      Successfully uninstalled datasets-3.0.1\n",
      "Successfully installed datasets-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efa5e3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\sarah/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\sarah/.cache\\\\torch\\\\hub\\\\huggingface_pytorch-transformers_main\\\\hubconf.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhuggingface/pytorch-transformers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodelForSequenceClassification\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# Download model and configuration from S3 and cache.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuggingface/pytorch-transformers\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodelForSequenceClassification\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./test/bert_model/\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# E.g. model was saved using `save_pretrained('./test/saved_model/')`\u001b[39;00m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuggingface/pytorch-transformers\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodelForSequenceClassification\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m, output_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Update configuration during loading\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\hub.py:566\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    563\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    564\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[1;32m--> 566\u001b[0m model \u001b[38;5;241m=\u001b[39m _load_local(repo_or_dir, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\hub.py:592\u001b[0m, in \u001b[0;36m_load_local\u001b[1;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _add_to_sys_path(hubconf_dir):\n\u001b[0;32m    591\u001b[0m     hubconf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hubconf_dir, MODULE_HUBCONF)\n\u001b[1;32m--> 592\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m \u001b[43m_import_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODULE_HUBCONF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhubconf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[0;32m    595\u001b[0m     model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\hub.py:106\u001b[0m, in \u001b[0;36m_import_module\u001b[1;34m(name, path)\u001b[0m\n\u001b[0;32m    104\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mmodule_from_spec(spec)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec\u001b[38;5;241m.\u001b[39mloader, Loader)\n\u001b[1;32m--> 106\u001b[0m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:846\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:982\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1039\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\sarah/.cache\\\\torch\\\\hub\\\\huggingface_pytorch-transformers_main\\\\hubconf.py'"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('huggingface/pytorch-transformers', 'modelForSequenceClassification', 'bert-base-uncased')    # Download model and configuration from S3 and cache.\n",
    "model = torch.hub.load('huggingface/pytorch-transformers', 'modelForSequenceClassification', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n",
    "model = torch.hub.load('huggingface/pytorch-transformers', 'modelForSequenceClassification', 'bert-base-uncased', output_attention=True)  # Update configuration during loading\n",
    "assert model.config.output_attention == True\n",
    "# Loading from a TF checkpoint file instead of a PyTorch model (slower)\n",
    "config = AutoConfig.from_json_file('./tf_model/bert_tf_model_config.json')\n",
    "model = torch.hub.load('huggingface/pytorch-transformers', 'modelForSequenceClassification', './tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e4b6700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\sarah/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\sarah/.cache\\\\torch\\\\hub\\\\huggingface_pytorch-transformers_main\\\\hubconf.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mpytorch including multipe languages\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m sequence_classification_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhuggingface/pytorch-transformers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodelForSequenceClassification\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-cased-finetuned-mrpc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m sequence_classification_tokenizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuggingface/pytorch-transformers\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-cased-finetuned-mrpc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\hub.py:566\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    563\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    564\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[1;32m--> 566\u001b[0m model \u001b[38;5;241m=\u001b[39m _load_local(repo_or_dir, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\hub.py:592\u001b[0m, in \u001b[0;36m_load_local\u001b[1;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _add_to_sys_path(hubconf_dir):\n\u001b[0;32m    591\u001b[0m     hubconf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hubconf_dir, MODULE_HUBCONF)\n\u001b[1;32m--> 592\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m \u001b[43m_import_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODULE_HUBCONF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhubconf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[0;32m    595\u001b[0m     model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\hub.py:106\u001b[0m, in \u001b[0;36m_import_module\u001b[1;34m(name, path)\u001b[0m\n\u001b[0;32m    104\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mmodule_from_spec(spec)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec\u001b[38;5;241m.\u001b[39mloader, Loader)\n\u001b[1;32m--> 106\u001b[0m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:846\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:982\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1039\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\sarah/.cache\\\\torch\\\\hub\\\\huggingface_pytorch-transformers_main\\\\hubconf.py'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "pytorch including multipe languages\n",
    "'''\n",
    "sequence_classification_model = torch.hub.load('huggingface/pytorch-transformers', 'modelForSequenceClassification', 'bert-base-cased-finetuned-mrpc')\n",
    "sequence_classification_tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased-finetuned-mrpc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ad648",
   "metadata": {},
   "source": [
    "link pytorch transformer [https://pytorch.org/hub/huggingface_pytorch-transformers/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c63df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
