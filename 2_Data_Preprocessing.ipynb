{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff01a6f3",
   "metadata": {},
   "source": [
    "#### Preparing data for the training\n",
    "\n",
    "For training the text classification model, I use each speach as a sample and each party affiliation as a tag.\n",
    "Before that, I want to look at the quality of the speeches. Some might be too short or too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "245964b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import Libraries'''\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle as pkl\n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Sampler, BatchSampler, Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba69af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_f1(train_f1, dev_f1, threshold=0.0015):\n",
    "    \"\"\"\n",
    "    Returns development F1 if overfitting is below threshold, otherwise 0.\n",
    "    \"\"\"\n",
    "    return dev_f1 if (train_f1 - dev_f1) < threshold else 0\n",
    "\n",
    "\n",
    "def save_metrics(*args, path, fname):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    if not os.path.isfile(path + fname):\n",
    "        with open(path + fname, \"w\", newline=\"\\n\") as f:\n",
    "            f.write(\n",
    "                \",\".join(\n",
    "                    [\n",
    "                        \"config\",\n",
    "                        \"epoch\",\n",
    "                        \"train_loss\",\n",
    "                        \"train_acc\",\n",
    "                        \"train_f1\",\n",
    "                        \"val_loss\",\n",
    "                        \"val_acc\",\n",
    "                        \"val_f1\",\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            f.write(\"\\n\")\n",
    "    if args:\n",
    "        with open(path + fname, \"a\", newline=\"\\n\") as f:\n",
    "            f.write(\",\".join([str(arg) for arg in args]))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6c7523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f7bd0ebef0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = 5_000 #20_000\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 15\n",
    "MAX_LEN = 128 #256\n",
    "LEARNING_RATE = 1e-4\n",
    "seed_value = 43\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e087b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b51e54b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= df[:100]\n",
    "dev_data= df[101:120]\n",
    "test_data= df[121:140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb3caa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyVocab:\n",
    "    def __init__(self, spacy_model_name=\"de_core_news_md\", max_vocab_size=10000, specials=['<UNK>', '<PAD>']):\n",
    "        # Load the spaCy model. Disable parts of pipeline that we don't need to speed up processing\n",
    "        self.nlp = spacy.load(spacy_model_name, disable=[\"ner\", \"parser\", \"tagger\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.specials = specials\n",
    "        self.vocab = {}\n",
    "        self.unk_index = None\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        # Tokenize sentences in bulk using spaCy's pipe method\n",
    "        tokenized_sentences = list(self.nlp.pipe(sentences))\n",
    "\n",
    "        # Add special tokens to the vocabulary and set the unknown token index?\n",
    "        self.vocab = {token: idx for idx, token in enumerate(self.specials)}\n",
    "\n",
    "\n",
    "        # Flatten the tokenized sentences and count word frequencies\n",
    "        token_counts = Counter(token.text for doc in tokenized_sentences for token in doc)\n",
    "\n",
    "        #making sure the most occuring tokens appea in the vocabulary:\n",
    "\n",
    "        most_common_tokens = token_counts.most_common(self.max_vocab_size - len(self.specials))\n",
    "\n",
    "        #adding the most frequent tokens to the vocabulary\n",
    "        for idx, (token, _) in enumerate(most_common_tokens, start=len(self.specials)):\n",
    "            self.vocab[token] = idx\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, sentence, max_len):\n",
    "\n",
    "        #tokenizing the input sentence\n",
    "        tokenized_sentence = self.nlp(sentence)\n",
    "\n",
    "        # Convert text to token indices using the vocabulary, defaulting to <UNK> index for unknown tokens\n",
    "        token_indices = [self.vocab.get(token.text,  self.vocab['<UNK>']) for token in tokenized_sentence]\n",
    "\n",
    "\n",
    "        if len(token_indices) > max_len:\n",
    "            token_indices = token_indices[:max_len]  #shortening the list of token indexes if it exceedes max_len\n",
    "\n",
    "        return token_indices\n",
    "\n",
    "# Create an instance of SpacyVocab with the specified spaCy model\n",
    "spacy_vocab = SpacyVocab()\n",
    "\n",
    "# Build the vocabulary using the training data\n",
    "spacy_vocab.build_vocab(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d624eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = [spacy_vocab(sentence, MAX_LEN) for sentence in train_data['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "074e8e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_idx  = [spacy_vocab(sentence, MAX_LEN) for sentence in dev_data['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f57674f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = [spacy_vocab(sentence, MAX_LEN) for sentence in test_data['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddc7667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParliamentDataset(Dataset):\n",
    "    def __init__(self, seq, lbl):\n",
    "        self.seq = seq\n",
    "        self.lbl = lbl\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # seq -  a list of tokenized indexes\n",
    "        # lbl - the list of labels\n",
    "        return torch.tensor(self.seq[idx]), torch.tensor(self.lbl[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        #return the length of the sequence?\n",
    "        return len(self.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c0bfd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ParliamentDataset(train_idx, train_data['party'])\n",
    "dev_set = ParliamentDataset(dev_idx, dev_data['party'])\n",
    "test_set = ParliamentDataset(test_idx, test_data['party'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1b3d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedSampler(Sampler):\n",
    "    def __init__(self, seqs, batch_size):\n",
    "        self.seqs = seqs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        #pairing each sequence index with its length: [(index, length), ...]\n",
    "        self.lengths = [(i, len(seq)) for i, seq in enumerate(seqs)]\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Creates an iterator that returns shuffled indices sorted by length within chunks of size batch_size * 100.\n",
    "        \"\"\"\n",
    "        #shuffling the list of (index, length) tuples\n",
    "        random.shuffle(self.lengths)\n",
    "\n",
    "        #setting group size as batch_size * 100\n",
    "        group_size = self.batch_size * 100\n",
    "\n",
    "        #sorting the data by sequence length in each group\n",
    "        grouped_sorted = []\n",
    "        for i in range(0, len(self.lengths), group_size):\n",
    "            group = self.lengths[i:i + group_size]\n",
    "            #sort group by the second element\n",
    "            grouped_sorted.extend(sorted(group, key=lambda x: x[1]))\n",
    "\n",
    "        # print(grouped_sorted)\n",
    "        #Return only the indices, maintaining the sorted order within each group\n",
    "        indices = [index for index, _ in grouped_sorted]\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d649d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped_sampler = GroupedSampler(train_idx, BATCH_SIZE)\n",
    "train_sampler =  BatchSampler(train_grouped_sampler, batch_size=BATCH_SIZE, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50b6f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    sequences, labels = zip(*batch)  # Batch is a list of (sequence, label) pairs\n",
    "\n",
    "    #original lengths of sequences (before padding)\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)\n",
    "\n",
    "    #sequences to the same length using padding_value=1\n",
    "    padded_sequences = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in sequences],\n",
    "                                    batch_first=True,\n",
    "                                    padding_value=1)\n",
    "\n",
    "    #labels to tensor\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return padded_sequences, labels, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bc5ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "NUM_WORKERS = 1 #it was said it should be <=2, I guess 0 is okay\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_sampler=train_sampler, collate_fn=collate_batch, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "# DataLoadesr for the rest of the sets withouut shuffling or sampling as they are used for evaluation only\n",
    "dev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        collate_fn=collate_batch, num_workers=NUM_WORKERS)\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                         collate_fn=collate_batch, num_workers=NUM_WORKERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd90d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
